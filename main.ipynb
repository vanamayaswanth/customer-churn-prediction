{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(\"customer_churn_large_dataset.xlsx\")\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "\n",
    "# 1.1 Handling Missing Data (if any)\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "\n",
    "# 1.2 Encode Categorical Variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Location'] = label_encoder.fit_transform(data['Location'])\n",
    "data['Gender'] = label_encoder.fit_transform(data['Gender'])\n",
    "\n",
    "\n",
    "# 1.3 Split the data into features (X) and the target variable (y)\n",
    "X = data.drop(columns=['CustomerID', 'Name', 'Churn'])\n",
    "y = data['Churn']\n",
    "\n",
    "# 1.4 Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X[['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']] = scaler.fit_transform(\n",
    "    X[['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Selection/Engineering\n",
    "\n",
    "# 2.1 Identify Relevant Features\n",
    "# You can use feature importance from a tree-based model (e.g., Random Forest) to identify important features.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model to the data to get feature importances\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Get feature importances and map them to feature names\n",
    "feature_importances = pd.Series(rf_classifier.feature_importances_, index=X.columns)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "\n",
    "# Select the top N important features (you can adjust N as needed)\n",
    "N = 5\n",
    "selected_features = feature_importances.head(N).index\n",
    "X_selected = X[selected_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/home/yashu/Desktop/customer churn prediction /ish/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create dictionaries to store the evaluation metrics for each model\n",
    "accuracy_scores = {}\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "roc_auc_scores = {}\n",
    "\n",
    "# Step 6: Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_scores[model_name] = accuracy_score(y_test, y_pred)\n",
    "    precision_scores[model_name] = precision_score(y_test, y_pred)\n",
    "    recall_scores[model_name] = recall_score(y_test, y_pred)\n",
    "    f1_scores[model_name] = f1_score(y_test, y_pred)\n",
    "    roc_auc_scores[model_name] = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Print evaluation metrics for each model\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_scores[model_name]:.4f}\")\n",
    "    print(f\"Precision: {precision_scores[model_name]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[model_name]:.4f}\")\n",
    "    print(f\"F1-Score: {f1_scores[model_name]:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_scores[model_name]:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained models to pickle files\n",
    "for model_name, model in models.items():\n",
    "    with open(f'{model_name}_model.pkl', 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
